{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarthikGowdaRamakrishna/Adv-Techniques-With-LLM-INFO7374-Spring-2025/blob/main/Assignment_2_Karthik.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Bigram Language Model and Generative Pretrained Transformer (GPT)\n",
        "\n",
        "\n",
        "The objective of this assignment is to train a simplified transformer model. The primary differences between the implementation:\n",
        "* tokenizer (we use a character level encoder simplicity and compute constraints)\n",
        "* size (we are using 1 consumer grade gpu hosted on colab and a small dataset. in practice, the models are much larger and are trained on much more data)\n",
        "* efficiency\n",
        "\n",
        "\n",
        "Most modern LLMs have multiple training stages, so we won't get a model that is capable of replying to you yet. However, this is the first step towards a model like ChatGPT and Llama.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fYCVVv_1A1kZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F097yaiu7dXQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Bigram MLP for TinyShakespeare (35 points)\n",
        "\n",
        "1a) (1 point). Create a list `chars` that contains all unique characters in `text`\n",
        "\n",
        "1b) (2 points). Implement `encode(s: str) -> list[int]`\n",
        "\n",
        "1c) (2 points). Implement `decode(ids: list[int]) -> str`\n",
        "\n",
        "1d) (5 points). Create two tensors, `inputs_one_hot` and `outputs_one_hot`. Use one hot encoding. Make sure to get every consecutive pair of characters. For example, for the word 'hello', we should create the following input-output pairs\n",
        "```\n",
        "he\n",
        "el\n",
        "ll\n",
        "lo\n",
        "```\n",
        "\n",
        "1e) (10 points). Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token. Specifically, implement the constructor, forward, and generate. The output dimension of the first layer should be 8. Use `torch.optim`. The activation function for the first layer should be `nn.LeakyReLU()`\n",
        "\n",
        "Note: Use the `torch.nn.function.cross_entropy` loss. Read the [docs](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) about how this loss function works. The logits are the output of a network WITHOUT an activation function applied to the last layer. There are activation functions are applied to every layer except the last.\n",
        "\n",
        "1f) (5 points). Train the BigramOneHotMLP for 1000 steps.\n",
        "\n",
        "1g) (5 points). Create two tensors, `input_ids` and `outputs_one_hot`. These `input_ids` will be used for the embedding layer.\n",
        "\n",
        "1h) (5 points). Implement and train BigramEmbeddingMLP, a 2 layer mlp that predicts the next token. Specifically, implement the constructor, forward, and generate functions. The output dimension of the first layer should be 8. Use `torch.optim`.\n",
        "\n",
        "\n",
        "\n",
        "Note: the output will look like gibberish\n"
      ],
      "metadata": {
        "id": "8qra06Ema_VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iradmn7bZtM",
        "outputId": "39d34ed4-b2d7-4f34-9cea-13eca58a6101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-28 05:08:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-02-28 05:08:38 (29.5 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the bigram model, let's use the first 1000 characters for the data\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "text = text[:1000]"
      ],
      "metadata": {
        "id": "pLoVi294G-T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1a) Collect all unique characters"
      ],
      "metadata": {
        "id": "8zL-FqKf2jG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(\"Unique characters found:\", chars)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "uouy1bun2ijG",
        "outputId": "ed377017-0a54-4f1e-b315-2976e24f7eb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique characters found: ['\\n', ' ', '!', \"'\", ',', '.', ':', ';', '?', 'A', 'B', 'C', 'F', 'I', 'L', 'M', 'N', 'O', 'R', 'S', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n",
            "Vocab size: 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1b) Implement encode(s: str) -> list[int]"
      ],
      "metadata": {
        "id": "4s2dSvjv20mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stoi = { ch: i for i, ch in enumerate(chars) }  # string to integer\n",
        "itos = { i: ch for i, ch in enumerate(chars) }  # integer to string\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Convert a string into a list of integer indices.\n",
        "    \"\"\"\n",
        "    return [stoi[ch] for ch in s]\n"
      ],
      "metadata": {
        "id": "D5ExdJD-2eXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1c) Implement decode(ids: list[int]) -> str"
      ],
      "metadata": {
        "id": "57QfOcpf3Lus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(ids: list[int]) -> str:\n",
        "    \"\"\"\n",
        "    Convert a list of integer indices back to the original string.\n",
        "    \"\"\"\n",
        "    return ''.join(itos[i] for i in ids)\n"
      ],
      "metadata": {
        "id": "b0pM15eO3PXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1d) Create two tensors, inputs_one_hot and outputs_one_hot"
      ],
      "metadata": {
        "id": "dLRv4Kat3-Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Encode the text\n",
        "encoded_text = encode(text)\n",
        "\n",
        "x_list = []\n",
        "y_list = []\n",
        "\n",
        "# consecutive pair (for 'hello', we get 'h' -> 'e', 'e' -> 'l', ...)\n",
        "for i in range(len(encoded_text) - 1):\n",
        "    x_list.append(encoded_text[i])\n",
        "    y_list.append(encoded_text[i + 1])\n",
        "\n",
        "# Convert to torch tensors\n",
        "x = torch.tensor(x_list, dtype=torch.long)  # shape [N]\n",
        "y = torch.tensor(y_list, dtype=torch.long)  # shape [N]\n",
        "\n",
        "# Create one-hot encodings\n",
        "N = x.shape[0]\n",
        "inputs_one_hot = torch.zeros(N, vocab_size)\n",
        "inputs_one_hot[torch.arange(N), x] = 1.0\n",
        "\n",
        "outputs_one_hot = torch.zeros(N, vocab_size)\n",
        "outputs_one_hot[torch.arange(N), y] = 1.0\n",
        "\n",
        "print(\"inputs_one_hot shape:\", inputs_one_hot.shape)\n",
        "print(\"outputs_one_hot shape:\", outputs_one_hot.shape)\n"
      ],
      "metadata": {
        "id": "Phg5z6EY3z9-",
        "outputId": "1f8eaf56-9b75-48de-f726-1635bf70cf37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_one_hot shape: torch.Size([999, 46])\n",
            "outputs_one_hot shape: torch.Size([999, 46])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1e) Implement BigramOneHotMLP"
      ],
      "metadata": {
        "id": "uYfQHkKf3ztV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class BigramOneHotMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=8):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(vocab_size, hidden_dim),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_one_hot):\n",
        "\n",
        "        logits = self.net(x_one_hot)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, start_idx, max_new_tokens=50):\n",
        "\n",
        "        generated = [start_idx]\n",
        "        current_one_hot = torch.zeros(vocab_size)\n",
        "        current_one_hot[start_idx] = 1.0\n",
        "        current_one_hot = current_one_hot.unsqueeze(0)  # shape [1, vocab_size]\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.forward(current_one_hot)  # [1, vocab_size]\n",
        "            # Convert logits to probabilities\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            # probability distribution\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated.append(next_idx)\n",
        "\n",
        "            current_one_hot = torch.zeros_like(current_one_hot)\n",
        "            current_one_hot[0, next_idx] = 1.0\n",
        "\n",
        "        return generated\n"
      ],
      "metadata": {
        "id": "e3YcJvLj4cwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1f) Train the BigramOneHotMLP for 1000 steps"
      ],
      "metadata": {
        "id": "XU-x4M0F5mH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = BigramOneHotMLP(vocab_size=vocab_size, hidden_dim=8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "inputs_one_hot_float = inputs_one_hot.float()\n",
        "\n",
        "num_steps = 1000\n",
        "for step in range(num_steps):\n",
        "    # Forward pass\n",
        "    logits = model(inputs_one_hot_float)  # shape: [N, vocab_size]\n",
        "\n",
        "    # raw logits and integer targets.\n",
        "    loss = criterion(logits, y)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step} / {num_steps}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "FcuEqbCU5otl",
        "outputId": "da66a30d-aecd-4b83-cfca-19de9b5c0bd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0 / 1000, Loss: 3.8279\n",
            "Step 100 / 1000, Loss: 2.3991\n",
            "Step 200 / 1000, Loss: 2.2108\n",
            "Step 300 / 1000, Loss: 2.1592\n",
            "Step 400 / 1000, Loss: 2.1394\n",
            "Step 500 / 1000, Loss: 2.1265\n",
            "Step 600 / 1000, Loss: 2.1174\n",
            "Step 700 / 1000, Loss: 2.1106\n",
            "Step 800 / 1000, Loss: 2.1055\n",
            "Step 900 / 1000, Loss: 2.1015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1g) Create input_ids and outputs_one_hot"
      ],
      "metadata": {
        "id": "8HoHS_Kv6CkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "input_ids = x  # integer IDs for the input\n",
        "outputs_one_hot = torch.zeros_like(inputs_one_hot)  # or reconstruct similarly\n",
        "outputs_one_hot[torch.arange(len(y)), y] = 1.0\n",
        "\n",
        "print(\"input_ids shape:\", input_ids.shape)        # Should be [N]\n",
        "print(\"outputs_one_hot shape:\", outputs_one_hot.shape)  # Should be [N, vocab_size]\n"
      ],
      "metadata": {
        "id": "yqCwcTHC6Gc8",
        "outputId": "e34ed1a8-23ea-473f-f2d0-32fa1cfec821",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids shape: torch.Size([999])\n",
            "outputs_one_hot shape: torch.Size([999, 46])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1h) Implement and train BigramEmbeddingMLP"
      ],
      "metadata": {
        "id": "xLcwOM1q6Rr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=8, hidden_dim=8):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_ids):\n",
        "        \"\"\"\n",
        "        x_ids: [batch_size] of integer token IDs\n",
        "        Returns logits: [batch_size, vocab_size]\n",
        "        \"\"\"\n",
        "        x_embed = self.embedding(x_ids)        # shape: [batch_size, embed_dim]\n",
        "        logits = self.net(x_embed)             # shape: [batch_size, vocab_size]\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, start_idx, max_new_tokens=50):\n",
        "        \"\"\"\n",
        "        start_idx: single integer index representing the initial character\n",
        "        max_new_tokens: how many characters to generate\n",
        "        \"\"\"\n",
        "        generated_indices = [start_idx]\n",
        "        current_idx = torch.tensor([start_idx], dtype=torch.long)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.forward(current_idx)     # [1, vocab_size]\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated_indices.append(next_idx)\n",
        "\n",
        "            current_idx = torch.tensor([next_idx], dtype=torch.long)\n",
        "\n",
        "        return generated_indices\n",
        "\n",
        "# Now let's train the model\n",
        "model_embed = BigramEmbeddingMLP(vocab_size=vocab_size, embed_dim=8, hidden_dim=8)\n",
        "optimizer_embed = torch.optim.Adam(model_embed.parameters(), lr=1e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_steps = 1000\n",
        "for _ in range(num_steps):\n",
        "    logits = model_embed(input_ids)  # shape: [N, vocab_size]\n",
        "\n",
        "    loss = criterion(logits, y)\n",
        "\n",
        "    # Backprop\n",
        "    optimizer_embed.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_embed.step()\n",
        "\n",
        "    if _ % 100 == 0:\n",
        "        print(f\"Step {_}/{num_steps}, loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "YBCsi9db6WoI",
        "outputId": "2793e797-cb35-4265-ddfc-5a635e896225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0/1000, loss: 3.8591\n",
            "Step 100/1000, loss: 2.4054\n",
            "Step 200/1000, loss: 2.2370\n",
            "Step 300/1000, loss: 2.1792\n",
            "Step 400/1000, loss: 2.1550\n",
            "Step 500/1000, loss: 2.1396\n",
            "Step 600/1000, loss: 2.1291\n",
            "Step 700/1000, loss: 2.1228\n",
            "Step 800/1000, loss: 2.1181\n",
            "Step 900/1000, loss: 2.1142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding_inputs_and_outputs() -> list[torch.tensor, torch.tensor]:\n",
        "    # implement\n",
        "    pass\n",
        "\n",
        "input_ids, outputs_one_hot = create_embedding_inputs_and_outputs()\n",
        "\n",
        "class BigramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        # implement\n",
        "        pass\n",
        "    def forward(self, x):\n",
        "        # implement\n",
        "        pass\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        # implement\n",
        "        pass\n",
        "\n",
        "bigram_embedding_mlp = BigramEmbeddingMLP()\n",
        "\n",
        "# training loop\n",
        "for _ in range(1000):\n",
        "    # implement\n",
        "    pass\n",
        "\n",
        "\n",
        "print(bigram_embedding_mlp.generate())"
      ],
      "metadata": {
        "id": "PasrfDz-dSqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Generative Pretrained Transformer (65 points)\n",
        "\n",
        "For this part, it is best to use a gpu. In the settings at the top go to Runtime -> Change Runtime Type and select T4 GPU"
      ],
      "metadata": {
        "id": "qplpM8_Cbp0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run nvidia-smi to check gpu usage\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0Oh-3FeFxxnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f529418d-ca91-4347-8b5d-026bf8f42f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb 28 05:08:21 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the gpt model, let's use the full text\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "rhJAwCAOADP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a character level tokenization function.\n",
        "\n",
        "1. Create a list of unique characters in the string. (1 points)\n",
        "2. Implement a function `encode(s: str) -> list[int]` that takes a string and returns a list of ids (1 point)\n",
        "3. Implement a function `decode(ids: list[int]) -> str` that takes a list of ids (ints) and returns a string (1 point)\n"
      ],
      "metadata": {
        "id": "z_LZpvZ8AEEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Number of unique characters:\", vocab_size)\n",
        "print(\"Unique characters:\", chars)\n",
        "\n",
        "# mappings for encode/decode\n",
        "stoi = { ch: i for i, ch in enumerate(chars)}  # string to integer\n",
        "itos = { i: ch for i, ch in enumerate(chars)}  # integer to string\n",
        "\n",
        "# 4) Implement encode(s: str) -> list[int]\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [stoi[ch] for ch in s]\n",
        "\n",
        "# 5) Implement decode(ids: list[int]) -> str\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join(itos[i] for i in ids)\n",
        "\n",
        "# Quick test\n",
        "sample = \"Hello\"\n",
        "encoded_sample = encode(sample)\n",
        "decoded_sample = decode(encoded_sample)\n",
        "print(f\"Sample: '{sample}' -> {encoded_sample} -> '{decoded_sample}'\")\n"
      ],
      "metadata": {
        "id": "rnEOfMj4Dk4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b844c1-e4e6-44fc-883e-ddca7bff541e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique characters: 65\n",
            "Unique characters: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "Sample: 'Hello' -> [20, 43, 50, 50, 53] -> 'Hello'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long).cuda()"
      ],
      "metadata": {
        "id": "1gyOaRF5Dq1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 16\n",
        "data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvWGi8Mk6x1q",
        "outputId": "1d37b177-d82f-4f88-91d2-40b697173441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a transformer, we feed the model `n` tokens (context) and try to predict the `n+1`th token (target) in the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "GvO4hSK171Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = data[:block_size]\n",
        "y = data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVWxO6Pa70Lh",
        "outputId": "e832eb90-61f9-43c5-ae7c-074930c08515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18], device='cuda:0') the target: 47\n",
            "when input is tensor([18, 47], device='cuda:0') the target: 56\n",
            "when input is tensor([18, 47, 56], device='cuda:0') the target: 57\n",
            "when input is tensor([18, 47, 56, 57], device='cuda:0') the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58], device='cuda:0') the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1], device='cuda:0') the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15], device='cuda:0') the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47], device='cuda:0') the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58], device='cuda:0') the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47], device='cuda:0') the target: 64\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64], device='cuda:0') the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43], device='cuda:0') the target: 52\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52], device='cuda:0') the target: 10\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10],\n",
            "       device='cuda:0') the target: 0\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0],\n",
            "       device='cuda:0') the target: 14\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14],\n",
            "       device='cuda:0') the target: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "lFYZnm2MuLlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Self Attention Head (5 points)\n",
        "![](https://i.ibb.co/GWR1XG0/head.png)"
      ],
      "metadata": {
        "id": "-HmnXJjxtm3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SingleHeadSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, block_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "        # Causal mask: we only attend to positions <= current index\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Tensor of shape (B, T, C)\n",
        "           B = batch size\n",
        "           T = block_size (sequence length)\n",
        "           C = embed_dim (embedding dimension)\n",
        "        \"\"\"\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Compute key, query, value\n",
        "        k = self.key(x)   # (B, T, C)\n",
        "        q = self.query(x) # (B, T, C)\n",
        "        v = self.value(x) # (B, T, C)\n",
        "\n",
        "        # Compute attention scores: (B, T, C) x (B, T, C)^T -> (B, T, T)\n",
        "        # We'll transpose the last two dimensions of k for the matrix multiply\n",
        "        att = q @ k.transpose(-2, -1) / (C**0.5)\n",
        "\n",
        "        # Apply the causal mask to prevent attending to future tokens\n",
        "        att = att.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        # Weighted sum of the values\n",
        "        out = att @ v  # (B, T, T) x (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "5SD8Z16R-sfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Self Attention (5 points)\n",
        "\n",
        "`constructor`\n",
        "\n",
        "- Create 4 `SelfAttentionHead` instances. Consider using `nn.ModuleList`\n",
        "- Create a linear layer with n_embd input dim and n_embd output dim\n",
        "\n",
        "`forward`\n",
        "\n",
        "In the forward implementation, pass `x` through each head, then concatenate all the outputs along the feature dimension, then pass the concatenated output through the linear layer\n",
        "\n",
        "![](https://i.ibb.co/y5SwyZZ/multihead.png)"
      ],
      "metadata": {
        "id": "LWeoHGBiFpWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MinimalAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, block_size):\n",
        "        super().__init__()\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "        self.attn = SingleHeadSelfAttention(embed_dim, block_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Pre-norm approach: layer norm first, then attention\n",
        "        x_normed = self.ln(x)\n",
        "        attn_out = self.attn(x_normed)\n",
        "\n",
        "        # Residual connection: add the original x\n",
        "        out = x + attn_out\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "gFsPDkpnFs_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP (2 points)\n",
        "Implement a 2 layer MLP\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/C0DtrF5/ff.png)"
      ],
      "metadata": {
        "id": "uH_0ELyZ8YCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim=64, hidden_dim=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),  # (B, T, 64) → (B, T, 256)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim),  # (B, T, 256) → (B, T, 64)\n",
        "            nn.Dropout(dropout)  # Final output remains (B, T, 64)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "K96Z3kAv7lNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, seq_len, embed_dim = 8, 32, 64\n",
        "mlp = MLP(embed_dim=embed_dim)\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)  # Random input\n",
        "output = mlp(x)\n",
        "print(\"Output shape:\", output.shape)  # Should be (8, 32, 64)\n"
      ],
      "metadata": {
        "id": "QdCQ7o-AD85X",
        "outputId": "da34db8b-7fbe-4a81-fc1d-f2b72d85907d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([8, 32, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer block (20 points)\n",
        "\n",
        "Layer normalization help training stability by normalizing the outputs of neurons within a single layer across all features for each individual data point, not across a full batch or a specific feature.\n",
        "\n",
        "Dropout is a form of regularization to prevent overfitting.\n",
        "\n",
        "This is the diagram of a transformer block:\n",
        "\n",
        "![](https://i.ibb.co/X85C473/block.png)"
      ],
      "metadata": {
        "id": "bUFxuyf-JIxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, block_size):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)  # Norm before attention\n",
        "        self.attn = SingleHeadSelfAttention(embed_dim, block_size)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)  # Norm before MLP\n",
        "        self.mlp = MLP(embed_dim)  # Uses the MLP we just implemented\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # Self-attention with residual connection\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "\n",
        "        # Feed-forward MLP with residual connection\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "\n",
        "        return x  # Output shape remains (B, T, C)\n"
      ],
      "metadata": {
        "id": "xTDAd66KIvvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT\n",
        "\n",
        "`constructor` (5 points)\n",
        "\n",
        "1. create the token embedding table and the position embedding table\n",
        "2. create variable `self.blocks` that is a series of 4 `Block`s. The data will pass through each block sequentially. Consider using `nn.Sequential`\n",
        "3. create a layer norm layer\n",
        "4. create a linear layer for predicting the next token\n",
        "\n",
        "`forward(self, idx, targets=None)`. (5 points)\n",
        "\n",
        "`forward` takes a batch of context ids as input of size (B, T) and returns the logits and the loss, if targets is not None. If targets is None, return the logits and None.\n",
        "1. get the token by using the token embedding table created in the constructor\n",
        "2. create the position embeddings\n",
        "3. sum the token and position embeddings to get the model input\n",
        "4. pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "5. compute the loss\n",
        "\n",
        "`generate(start_char, max_new_tokens, top_p, top_k, temperature) -> str` (5 points)\n",
        "1. implement top p, top_k, and temperature for sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "SyFQXltDKNti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/n8sbQ0V/Screenshot-2024-01-23-at-8-59-08-PM.png)"
      ],
      "metadata": {
        "id": "0Xa2bh2XDdKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd=64, n_head=4, n_layers=4, block_size=32):\n",
        "\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Token and Position Embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # Stacking 4 Transformer Blocks\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[TransformerBlock(n_embd, block_size) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        # Layer Norm & Linear Head\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape  # Batch size, Sequence length\n",
        "\n",
        "        # token embeddings\n",
        "        tok_emb = self.token_embedding(idx)  # (B, T, n_embd)\n",
        "\n",
        "        # position embeddings\n",
        "        pos_ids = torch.arange(T, device=idx.device)\n",
        "        pos_emb = self.position_embedding(pos_ids)  # (T, n_embd)\n",
        "        pos_emb = pos_emb.unsqueeze(0)  # (1, T, n_embd)\n",
        "\n",
        "        x = tok_emb + pos_emb  # (B, T, n_embd)\n",
        "\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # LayerNorm and linear projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits = logits.view(B * T, -1)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = nn.CrossEntropyLoss()(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, start_char, max_new_tokens=50, top_p=0.9, top_k=10, temperature=1.0):\n",
        "\n",
        "        self.eval()  # Set model to evaluation mode\n",
        "\n",
        "        idx = torch.tensor([[stoi[start_char]]], dtype=torch.long)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "\n",
        "            # Get logits for next token\n",
        "            logits, _ = self.forward(idx_cond)\n",
        "\n",
        "            logits = logits[:, -1, :]  # (1, vocab_size)\n",
        "\n",
        "            logits = logits / temperature\n",
        "\n",
        "            if top_k > 0:\n",
        "                values, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < values[:, -1]] = -float(\"Inf\")\n",
        "\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "                sorted_indices_to_remove[:, 0] = False\n",
        "\n",
        "                logits[sorted_indices[sorted_indices_to_remove]] = -float(\"Inf\")\n",
        "\n",
        "            # Convert logits to probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from probability distribution\n",
        "            next_token = torch.multinomial(probs, num_samples=1)  # (1, 1)\n",
        "\n",
        "            # Append new token to sequence\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "        # Convert generated token indices back to text\n",
        "        return decode(idx[0].tolist())\n"
      ],
      "metadata": {
        "id": "8WT4oUN084ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop (15 points)\n",
        "\n",
        "implement training loop"
      ],
      "metadata": {
        "id": "Njzrwwiv-mfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(data, block_size):\n",
        "\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - block_size):\n",
        "        X.append(data[i : i + block_size])\n",
        "        Y.append(data[i + 1 : i + block_size + 1])  # Shifted by 1 token\n",
        "\n",
        "    return torch.stack(X), torch.stack(Y)\n",
        "\n",
        "block_size = 32\n",
        "data = torch.tensor(encode(text), dtype=torch.long)  # Convert text to tokenized tensor\n",
        "X, Y = build_dataset(data, block_size)\n",
        "\n",
        "print(\"Dataset shapes:\", X.shape, Y.shape)  # Should be (N, 32) each\n"
      ],
      "metadata": {
        "id": "_HQsYE9xIl0C",
        "outputId": "72cfd069-c78e-46e0-88a0-ba8f298c177b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shapes: torch.Size([1115362, 32]) torch.Size([1115362, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Ensure the model runs on GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32         # How many sequences per batch\n",
        "max_iters = 5000        # Number of training iterations\n",
        "learning_rate = 3e-4    # Standard for transformer training\n",
        "\n",
        "# Initialize the GPT model and move it to GPU\n",
        "model = GPT(vocab_size=vocab_size, n_embd=64, n_head=4, block_size=32).to(device)\n",
        "\n",
        "# Optimizer (AdamW is commonly used for transformers)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Function to sample a batch of training data\n",
        "def get_batch(X, Y, batch_size):\n",
        "    ix = torch.randint(0, len(X), (batch_size,))\n",
        "    xb = X[ix].to(device)  # Move batch to GPU\n",
        "    yb = Y[ix].to(device)\n",
        "    return xb, yb\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    # Get a random batch\n",
        "    xb, yb = get_batch(X, Y, batch_size)\n",
        "\n",
        "    # Forward pass: compute logits and loss\n",
        "    logits, loss = model(xb, targets=yb)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss every 100 iterations\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}/{max_iters}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n"
      ],
      "metadata": {
        "id": "qWtn2uTwYUrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e62aae-9409-4284-882b-b1e541593e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0/5000, Loss: 4.3477\n",
            "Iteration 100/5000, Loss: 3.1248\n",
            "Iteration 200/5000, Loss: 2.7105\n",
            "Iteration 300/5000, Loss: 2.6458\n",
            "Iteration 400/5000, Loss: 2.4971\n",
            "Iteration 500/5000, Loss: 2.4554\n",
            "Iteration 600/5000, Loss: 2.3747\n",
            "Iteration 700/5000, Loss: 2.3771\n",
            "Iteration 800/5000, Loss: 2.3790\n",
            "Iteration 900/5000, Loss: 2.3144\n",
            "Iteration 1000/5000, Loss: 2.3413\n",
            "Iteration 1100/5000, Loss: 2.2722\n",
            "Iteration 1200/5000, Loss: 2.1911\n",
            "Iteration 1300/5000, Loss: 2.1950\n",
            "Iteration 1400/5000, Loss: 2.2010\n",
            "Iteration 1500/5000, Loss: 2.1022\n",
            "Iteration 1600/5000, Loss: 2.1532\n",
            "Iteration 1700/5000, Loss: 2.1230\n",
            "Iteration 1800/5000, Loss: 2.0947\n",
            "Iteration 1900/5000, Loss: 2.0825\n",
            "Iteration 2000/5000, Loss: 2.0701\n",
            "Iteration 2100/5000, Loss: 2.0781\n",
            "Iteration 2200/5000, Loss: 2.0484\n",
            "Iteration 2300/5000, Loss: 2.0762\n",
            "Iteration 2400/5000, Loss: 2.0352\n",
            "Iteration 2500/5000, Loss: 2.0111\n",
            "Iteration 2600/5000, Loss: 2.0277\n",
            "Iteration 2700/5000, Loss: 2.0335\n",
            "Iteration 2800/5000, Loss: 1.9911\n",
            "Iteration 2900/5000, Loss: 2.0167\n",
            "Iteration 3000/5000, Loss: 1.9827\n",
            "Iteration 3100/5000, Loss: 2.0109\n",
            "Iteration 3200/5000, Loss: 1.9008\n",
            "Iteration 3300/5000, Loss: 1.9500\n",
            "Iteration 3400/5000, Loss: 1.9628\n",
            "Iteration 3500/5000, Loss: 2.0222\n",
            "Iteration 3600/5000, Loss: 1.9200\n",
            "Iteration 3700/5000, Loss: 1.9419\n",
            "Iteration 3800/5000, Loss: 1.9110\n",
            "Iteration 3900/5000, Loss: 1.8927\n",
            "Iteration 4000/5000, Loss: 1.9089\n",
            "Iteration 4100/5000, Loss: 1.8731\n",
            "Iteration 4200/5000, Loss: 1.9752\n",
            "Iteration 4300/5000, Loss: 2.0035\n",
            "Iteration 4400/5000, Loss: 1.9065\n",
            "Iteration 4500/5000, Loss: 1.9873\n",
            "Iteration 4600/5000, Loss: 1.8612\n",
            "Iteration 4700/5000, Loss: 1.8301\n",
            "Iteration 4800/5000, Loss: 1.8699\n",
            "Iteration 4900/5000, Loss: 1.8804\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text\n",
        "\n",
        "\n",
        "print some text that your model generates"
      ],
      "metadata": {
        "id": "zy3v8Nv7YVUa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5l4soWviWG5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}